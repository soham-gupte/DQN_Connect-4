{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Epsilon Decision Function for Exploration vs Exploitation\n",
    "\n",
    "In reinforcement learning, the **epsilon-greedy strategy** helps balance **exploration** (trying new actions) and **exploitation** (choosing the best-known action). Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Define epsilon decision function\n",
    "def epsilonDecision(epsilon):\n",
    "  action_decision = random.choices(['model','random'], weights = [1 - epsilon, epsilon])[0]\n",
    "  return action_decision\n",
    "epsilonDecision(epsilon = 0) # would always give 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Creating and Interacting with a ConnectX Environment\n",
    "\n",
    "In this section, we set up a **ConnectX** game environment using `kaggle_environments`, a powerful tool to simulate competitive reinforcement learning games on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_environments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_environments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate, make, utils\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m make(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectx\u001b[39m\u001b[38;5;124m\"\u001b[39m, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_environments'"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "import gym\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.render()\n",
    "#Creates a new random trainer\n",
    "trainer = env.train([None, 'negamax'])\n",
    "#Resets the board, shows initial state of all 0\n",
    "trainer.reset()['board']\n",
    "#Make a new action: play position 4\n",
    "new_obs, winner, state, info = trainer.step(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Building a Deep Q-Network (DQN) for ConnectX\n",
    "\n",
    "In this step, we define a Deep Q-Network (DQN) using TensorFlow/Keras to learn how to play ConnectX.\n",
    "\n",
    "### üìå Key Components:\n",
    "\n",
    "- **Number of Actions**: Set to 7 ‚Äî one for each column where a move can be made.\n",
    "- **State Representation**: The board is a 6x7 grid representing the current game state.\n",
    "\n",
    "### üß± Network Architecture:\n",
    "\n",
    "- **Input Layer**: Accepts the 6x7 board.\n",
    "- **Flatten Layer**: Converts the 2D grid into a 1D vector.\n",
    "- **Hidden Layers**: Seven fully connected layers with 50 neurons each and ReLU activation ‚Äî this allows the model to learn complex patterns and strategies.\n",
    "- **Output Layer**: A dense layer with 7 units (one per action), using a linear activation function to produce Q-values for each possible move.\n",
    "\n",
    "This DQN architecture is designed to map game states to action values, enabling the agent to select optimal moves based on learned experiences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of possible actions\n",
    "num_actions = 7\n",
    "#Number of different states\n",
    "num_states = [6,7]\n",
    "\n",
    "# Create the DQN\n",
    "import tensorflow as tf\n",
    "inputs = tf.keras.layers.Input(shape=(6, 7))\n",
    "flatten = tf.keras.layers.Flatten()(inputs)\n",
    "hidden_1 = tf.keras.layers.Dense(50, activation = 'relu')(flatten)\n",
    "hidden_2 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_1)\n",
    "hidden_3 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_2)\n",
    "hidden_4 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_3)\n",
    "hidden_5 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_4)\n",
    "hidden_6 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_5)\n",
    "hidden_7 = tf.keras.layers.Dense(50, activation = 'relu')(hidden_6)\n",
    "output = tf.keras.layers.Dense(num_actions, activation = \"linear\")(hidden_7) \n",
    "model = tf.keras.models.Model(inputs = [inputs], outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Agent Decision-Making, Experience Tracking, and Reward Logic\n",
    "\n",
    "This section includes key functions and classes to control how the agent interacts with the environment, chooses actions, and learns from experience.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ `getAction(model, observation, epsilon)`\n",
    "\n",
    "This function uses the **epsilon-greedy policy** to decide whether to:\n",
    "- **Explore** by choosing a random action, or\n",
    "- **Exploit** the current model by selecting the best action based on predicted Q-values.\n",
    "\n",
    "- The game state (observation) is reshaped and passed through the model.\n",
    "- Predictions are passed through a **softmax** function to obtain probability-like weights.\n",
    "- Based on the epsilon decision, the agent either selects:\n",
    "  - The action with the highest weight (`argmax`) or\n",
    "  - A random action from the available options.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≥ `Experience` Class\n",
    "\n",
    "This class is a **memory buffer** to store the agent‚Äôs gameplay data:\n",
    "\n",
    "- **`observations`**: Stores board states.\n",
    "- **`actions`**: Stores actions taken.\n",
    "- **`rewards`**: Stores rewards received.\n",
    "\n",
    "This experience data is essential for training the DQN using reinforcement learning techniques like experience replay.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `checkValid(obs, action)`\n",
    "\n",
    "Ensures that the selected action is **valid** (i.e., the column is not full). If an invalid action is detected, the agent randomly selects a valid column.\n",
    "\n",
    "- Helps prevent illegal moves that would break the game logic.\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ `getReward(winner, state)`\n",
    "\n",
    "Defines the **reward strategy** based on the game outcome:\n",
    "\n",
    "- **0**: Game is still ongoing.\n",
    "- **+50**: Player 1 (agent) wins.\n",
    "- **-50**: Player 2 (opponent) wins or the game is a draw.\n",
    "\n",
    "This reward structure helps reinforce winning moves and penalize losses or non-optimal outcomes, guiding the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "These utilities work together to drive the agent's decision-making, learning, and interaction with the ConnectX environment effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(model, observation, epsilon):\n",
    "  #Get the action based on greedy epsilon policy\n",
    "  action_decision = epsilonDecision(epsilon)\n",
    "  #Reshape the observation to fit in model\n",
    "  observation = np.array([observation])\n",
    "  #Get predictions\n",
    "  preds = model.predict(observation)\n",
    "  #Get the softmax activation of the logits\n",
    "  weights = tf.nn.softmax(preds).numpy()[0]\n",
    "  if action_decision == 'model':\n",
    "    action = np.argmax(weights)\n",
    "  if action_decision == 'random':\n",
    "    action = random.randint(0,6)\n",
    "  return int(action), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience:\n",
    "  def __init__(self):\n",
    "    self.clear() \n",
    "  def clear(self):\n",
    "    self.observations = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "  def store_experience(self, new_obs, new_act, new_reward):\n",
    "    self.observations.append(new_obs)\n",
    "    self.actions.append(new_act)\n",
    "    self.rewards.append(new_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if action is valid\n",
    "# #Requires reshape(6,7) format\n",
    "def checkValid(obs, action):\n",
    "  valid_actions = set([0,1,2,3,4,5,6])\n",
    "  if obs[0,action] != 0:\n",
    "    valid_actions = valid_actions - set([action])\n",
    "    action = random.choice(list(valid_actions))\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get reward\n",
    "def getReward(winner, state):\n",
    "  #Game not done wet\n",
    "  if not state:\n",
    "    reward = 0\n",
    "  if state: \n",
    "    #If player 1 wins\n",
    "    if winner == 1:\n",
    "      reward = 50\n",
    "    #If player 2 wins\n",
    "    if winner == -1:\n",
    "      reward = -50\n",
    "    if winner == 0:\n",
    "      reward = -50\n",
    "  return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Training Step: Updating the DQN\n",
    "\n",
    "The `train_step` function defines how the model is trained using collected gameplay experiences. It updates the agent‚Äôs neural network weights based on the **observed states**, **chosen actions**, and **received rewards**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ What Happens in Each Training Step?\n",
    "\n",
    "- **Forward Pass**: The current observations (states) are fed into the model to compute predicted logits (raw Q-values before softmax).\n",
    "  \n",
    "- **Loss Calculation**: \n",
    "  - Uses **sparse softmax cross-entropy** to compare predicted logits with actual actions taken.\n",
    "  - The loss is weighted by the reward received for each action, which encourages the network to favor actions that led to better outcomes.\n",
    "\n",
    "- **Gradient Calculation**: \n",
    "  - A `GradientTape` tracks operations to compute the gradient of the loss with respect to the model's trainable parameters.\n",
    "\n",
    "- **Backpropagation**: \n",
    "  - Gradients are applied using the specified optimizer to adjust model weights and improve future decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Why It Works:\n",
    "\n",
    "This training approach helps the model learn to assign **higher probabilities** (Q-values) to actions that lead to **higher rewards**, thus improving its performance over time.\n",
    "\n",
    "This is a simple yet effective reinforcement learning update mechanism tailored for discrete action environments like ConnectX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, observations, actions, rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "      #Propagate through the agent network\n",
    "        logits = model(observations)\n",
    "        softmax_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "        loss = tf.reduce_mean(softmax_cross_entropy * rewards)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Training the DQN Agent Against a Random Opponent\n",
    "\n",
    "In this section, we train **Player 1 (the model)** to play ConnectX against a simple **random agent (Player 2)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Setup:\n",
    "\n",
    "- **Environment**: Created using `kaggle_environments.make(\"connectx\")`.\n",
    "- **Optimizer**: Adam optimizer is used to update the model‚Äôs weights.\n",
    "- **Experience Buffer**: Stores observations, actions, and rewards during each episode.\n",
    "- **Epsilon-Greedy Policy**: \n",
    "  - Starts with full exploration (`epsilon = 1`)\n",
    "  - Gradually shifts toward exploitation by decaying `epsilon` each episode.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Training Loop Overview:\n",
    "\n",
    "1. **Episodes**: Runs 100 training episodes.\n",
    "2. **Game Initialization**:\n",
    "   - Sets up a new match against a random agent.\n",
    "   - Resets the board and clears stored experience.\n",
    "3. **Agent's Turn**:\n",
    "   - Chooses an action using the epsilon-greedy strategy.\n",
    "   - Ensures the action is valid (column is not full).\n",
    "   - Plays the action and observes the new board state.\n",
    "   - Calculates the reward based on the outcome.\n",
    "   - Stores the experience (observation, action, reward).\n",
    "4. **Episode End**:\n",
    "   - If the game ends, the model is updated via `train_step`.\n",
    "   - Tracks the number of wins over episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Why This Works:\n",
    "\n",
    "This loop teaches the model to gradually learn winning strategies by playing many games against a random agent. Over time:\n",
    "- The agent starts exploiting more (making smarter moves).\n",
    "- The model improves by adjusting to patterns in winning vs. losing moves.\n",
    "- The win rate should increase, as tracked in the `win_track` list.\n",
    "\n",
    "This forms the foundation of reinforcement learning training in a discrete environment like ConnectX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "#Train P1 (model) against random agent P2\n",
    "#Create the environment\n",
    "env = make(\"connectx\", debug=True)\n",
    "#Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Set up the experience storage\n",
    "exp = Experience()\n",
    "epsilon = 1\n",
    "epsilon_rate = 0.995\n",
    "wins = 0\n",
    "win_track = []\n",
    "for episode in tqdm.tqdm(range(100)):\n",
    "  #Set up random trainer\n",
    "  trainer = env.train([None, 'random'])\n",
    "  #First observation\n",
    "  obs = np.array(trainer.reset()['board']).reshape(6,7)\n",
    "  #Clear cache\n",
    "  exp.clear()\n",
    "  #Decrease epsilon over time if we want\n",
    "  epsilon = epsilon * epsilon_rate\n",
    "  #Set initial state\n",
    "  state = False\n",
    "  while not state:\n",
    "    #Get action\n",
    "    action, w = getAction(model, obs, epsilon)\n",
    "    #Check if action is valid\n",
    "    while True:\n",
    "      temp_action = action\n",
    "      action = checkValid(obs, temp_action)\n",
    "      if temp_action == action:\n",
    "        break\n",
    "    #Play the action and retrieve info\n",
    "    new_obs, winner, state, info = trainer.step(action)\n",
    "    obs = np.array(new_obs['board']).reshape(6,7)\n",
    "    #Get reward\n",
    "    reward = getReward(winner, state)\n",
    "    #Store experience\n",
    "    exp.store_experience(obs, action, reward)\n",
    "    #Break if game is over\n",
    "    if state:\n",
    "      #This would be where training step goes I think\n",
    "      if winner == 1:\n",
    "        wins += 1\n",
    "      win_track.append(wins)\n",
    "      train_step(model, optimizer = optimizer,\n",
    "                 observations = np.array(exp.observations),\n",
    "                 actions = np.array(exp.actions),\n",
    "                 rewards = exp.rewards)\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/kaggle/input/connect-4-model/keras/default/1/connect4_agent_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 6, 7], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_4'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/soham/Downloads/connect4_agent_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m test_model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 6, 7], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_4'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']"
     ]
    }
   ],
   "source": [
    "test_model = tf.keras.models.load_model(\"/home/soham/Downloads/connect4_agent_model.h5\")\n",
    "model = test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Play Connect 4 Against Your Trained AI!\n",
    "\n",
    "This script sets up a **playable Connect 4 game** where you (the human) face off against the **trained AI agent** using the DQN model.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Game Logic Components:\n",
    "\n",
    "- **Board Setup**:\n",
    "  - The game board is a 6x7 NumPy array initialized with zeros.\n",
    "  - Emoji representation (`‚ö™`, `üî¥`, `üü°`) makes the game more visual and user-friendly in the notebook.\n",
    "\n",
    "- **Game Mechanics**:\n",
    "  - Validates moves to ensure players only drop tokens in legal columns.\n",
    "  - Handles token placement and updates the board accordingly.\n",
    "  - Includes all winning condition checks: horizontal, vertical, and both diagonals.\n",
    "  - Detects full board for a draw outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† AI Integration:\n",
    "\n",
    "- **Model Prediction**:\n",
    "  - The trained model is loaded and used to predict Q-values for the current board state.\n",
    "  - Invalid moves are masked with `-inf` so the AI avoids illegal plays.\n",
    "  - The action with the highest Q-value among valid options is chosen.\n",
    "\n",
    "---\n",
    "\n",
    "### üéÆ Gameplay Loop:\n",
    "\n",
    "- **Turn-Based Play**:\n",
    "  - Human plays first by selecting a column via input.\n",
    "  - The AI responds immediately after with its move.\n",
    "  - The board is printed after every move to visualize progress.\n",
    "  - Game ends on a win or a draw.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Model Loading:\n",
    "\n",
    "- The model is loaded from:  \n",
    "  `/kaggle/working/connect4_agent_model_2.h5`\n",
    "\n",
    "Make sure this file exists in your notebook directory for the script to run properly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ How to Play:\n",
    "\n",
    "- Run the cell, and you'll be prompted to enter a column (0‚Äì6) for your move.\n",
    "- Watch the AI respond intelligently based on what it learned during training!\n",
    "\n",
    "This is a fun and interactive way to evaluate how well your DQN agent learned the game of Connect 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 14:47:20.475844: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 14:47:20.879160: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-16 14:47:20.879218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-16 14:47:20.880672: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-16 14:47:21.145479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-16 14:47:22.245670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m             game_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# model = load_model(\"/kaggle/working/connect4_agent_model_2.h5\")\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     play_game(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "ROWS = 6\n",
    "COLUMNS = 7\n",
    "\n",
    "def create_board():\n",
    "    return np.zeros((ROWS, COLUMNS), dtype=int)\n",
    "\n",
    "def print_board(board):\n",
    "    emoji_map = {0: \"‚ö™\", 1: \"üî¥\", 2: \"üü°\"}\n",
    "    print()\n",
    "    for row in board:\n",
    "        print(\" \".join(emoji_map[val] for val in row))\n",
    "    print(\"0Ô∏è‚É£ 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ 4Ô∏è‚É£ 5Ô∏è‚É£ 6Ô∏è‚É£\")\n",
    "\n",
    "def is_valid_location(board, col):\n",
    "    return board[0][col] == 0\n",
    "\n",
    "def get_next_open_row(board, col):\n",
    "    for r in reversed(range(ROWS)):\n",
    "        if board[r][col] == 0:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def drop_piece(board, row, col, piece):\n",
    "    board[row][col] = piece\n",
    "\n",
    "def winning_move(board, piece):\n",
    "    # Horizontal\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLUMNS - 3):\n",
    "            if np.all(board[r, c:c+4] == piece):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMNS):\n",
    "        for r in range(ROWS - 3):\n",
    "            if np.all(board[r:r+4, c] == piece):\n",
    "                return True\n",
    "    # Positive diagonal\n",
    "    for r in range(ROWS - 3):\n",
    "        for c in range(COLUMNS - 3):\n",
    "            if all(board[r+i][c+i] == piece for i in range(4)):\n",
    "                return True\n",
    "    # Negative diagonal\n",
    "    for r in range(3, ROWS):\n",
    "        for c in range(COLUMNS - 3):\n",
    "            if all(board[r-i][c+i] == piece for i in range(4)):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_board_full(board):\n",
    "    return np.all(board[0] != 0)\n",
    "\n",
    "def get_valid_actions(board):\n",
    "    return [c for c in range(COLUMNS) if is_valid_location(board, c)]\n",
    "\n",
    "def get_ai_move(model, board):\n",
    "    valid_moves = get_valid_actions(board)\n",
    "    state = board.copy()\n",
    "    state = state.reshape(1, ROWS, COLUMNS, 1).astype(\"float32\")\n",
    "\n",
    "    q_values = model.predict(state, verbose=0)[0]\n",
    "\n",
    "    # Mask invalid moves\n",
    "    for c in range(COLUMNS):\n",
    "        if c not in valid_moves:\n",
    "            q_values[c] = -np.inf\n",
    "\n",
    "    return int(np.argmax(q_values))\n",
    "\n",
    "def play_game(model):\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    print_board(board)\n",
    "\n",
    "    while not game_over:\n",
    "        # Player 1 (Human)\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            try:\n",
    "                col = int(input(\"Your turn (0-6): \"))\n",
    "                if 0 <= col < COLUMNS and is_valid_location(board, col):\n",
    "                    valid = True\n",
    "                else:\n",
    "                    print(\"Invalid column. Try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Enter a number between 0 and 6.\")\n",
    "        \n",
    "        row = get_next_open_row(board, col)\n",
    "        drop_piece(board, row, col, 1)\n",
    "        print_board(board)\n",
    "\n",
    "        if winning_move(board, 1):\n",
    "            print(\"You win!\")\n",
    "            game_over = True\n",
    "            break\n",
    "        elif is_board_full(board):\n",
    "            print(\"Draw!\")\n",
    "            break\n",
    "\n",
    "        # Player 2 (AI)\n",
    "        col = get_ai_move(model, board)\n",
    "        row = get_next_open_row(board, col)\n",
    "        drop_piece(board, row, col, 2)\n",
    "        print(f\"AI played column {col}\")\n",
    "        print_board(board)\n",
    "\n",
    "        if winning_move(board, 2):\n",
    "            print(\"AI wins!\")\n",
    "            game_over = True\n",
    "        elif is_board_full(board):\n",
    "            print(\"Draw!\")\n",
    "            game_over = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model = load_model(\"/kaggle/working/connect4_agent_model_2.h5\")\n",
    "    play_game(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 305847,
     "modelInstanceId": 285009,
     "sourceId": 340772,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
